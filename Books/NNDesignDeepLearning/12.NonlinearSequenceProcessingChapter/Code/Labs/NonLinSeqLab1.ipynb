{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NNDesignDeepLearning/NNDesignDeepLearning/blob/master/05.PythonChapter/Code/LabSolutions/PythonLab1_Solution.ipynb)",
   "id": "af91286f733c5acc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Nonlinear Sequence Processing Lab 1 Objective\n",
    "\n",
    "This objective of this lab is to help you experiment with training recurrent neural networks. You will begin with only Python and Numpy, in order to see the details of the operation, and then will move on to PyTorch. The experiments will involve:\n",
    "\n",
    "- Implementing a recurrent neural network using Python and Numpy.\n",
    "- Implementing a backpropagation training algorithm in Python and Numpy.\n",
    "- Testing your algorithms on a dynamic test problem.\n",
    "- Repeating the steps with PyTorch implementations.\n",
    "\n",
    "Some of the cells in this notebook are prefilled with working code. In addition, there will be cells with missing code (labeled `# TODO`), which you will need to complete. If you need additional cells, you can use the `Insert` menu at the top of the page.\n",
    "\n",
    "## Loading Modules\n",
    "\n",
    "We begin by loading some useful modules."
   ],
   "id": "6d504391221d2ab6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "id": "89076048a7ccb199"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RNN in Python/Numpy\n",
    "\n",
    "\n",
    "Next we implement a recurrent neural network in Python/Numpy. We will first implement some components that we will need later. The RNN has the following structure.\n",
    "\n",
    "![](RNN2.jpg)\n"
   ],
   "id": "6456befe2542e117"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test Problem - Counting Ones in a Window\n",
    "\n",
    "We first define the problem that we are going to train the network for. In this case we are going to train a network to count the number of ones that appear in a window. The input to the network will be a binary sequence of ones and zeros. If the number of ones that appears in the window is even, then the network will output a 1. If the number of ones that appears in the window is odd, then the network will output a 0. The following cell implements a method to generate a data set for training."
   ],
   "id": "f1314bc2a53186e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_counting_ones_data(num_samples, seq_length, window_size):\n",
    "    \"\"\"\n",
    "    Generate input and target data for the \"Counting Ones in a Window\" task.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): Number of training examples.\n",
    "        seq_length (int): Length of each binary sequence.\n",
    "        window_size (int): Number of past steps to consider for counting.\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Input sequences of shape (num_samples, seq_length, 1).\n",
    "        y (np.ndarray): Target sequences of shape (num_samples, seq_length, 1).\n",
    "    \"\"\"\n",
    "    # Generate random binary sequences (0s and 1s) as integers, then convert to float32\n",
    "    X = np.random.randint(0, 2, size=(num_samples, seq_length)).astype(np.float32)\n",
    "\n",
    "    # Initialize targets\n",
    "    y = np.zeros((num_samples, seq_length), dtype=np.float32)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for t in range(seq_length):\n",
    "            # Look back 'window_size' steps (or fewer if at the start)\n",
    "            start = max(0, t - window_size + 1)\n",
    "            window = X[i, start:t + 1]\n",
    "\n",
    "            # Count number of 1s in the window\n",
    "            count = np.sum(window)\n",
    "\n",
    "            # Target is 1 if count is even, else 0\n",
    "            y[i, t] = 1 if count % 2 == 0 else 0\n",
    "\n",
    "    return X, y"
   ],
   "id": "c78663a5175443c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sigmoid Activation Function\n",
    "Now we build up the parts of the network. First, we create the log sigmoid activation function, which we use in the final layer of the network. The output represents the probability that there were an even number of ones in the previous window. For numerical stability, we handle positive values and negative values differently. Also, we clip the net input, if it gets larger than 500, so as not to overflow the exponential function."
   ],
   "id": "b90f8a0b6423d758"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sigmoid(n):\n",
    "    \"\"\"\n",
    "    Numerically stable sigmoid function.\n",
    "\n",
    "    For numerical stability, we handle large positive and negative values differently:\n",
    "    - For n >= 0: sigmoid(n) = 1 / (1 + exp(-n))\n",
    "    - For n < 0: sigmoid(n) = exp(n) / (1 + exp(n))\n",
    "\n",
    "    This prevents overflow in the exponential function.\n",
    "    \"\"\"\n",
    "    # TODO: Implement a numerically stable sigmoid function\n",
    "    # 1. Clip n to prevent overflow\n",
    "    # 2. Handle positive and negative values differently for numerical stability\n",
    "    # 3. Return the result\n",
    "    \n",
    "    return None"
   ],
   "id": "912ee5e449653ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Binary Cross-Entropy Loss\n",
    "\n",
    "Next, we create the loss function. We will be using Binary Cross-Entropy as our loss function. This is implemented in the following cell. The equation for BCE loss is:\n",
    "$$\\mathcal{L} = -\\frac{1}{Q} \\sum_{i=1}^Q \\left[ t(i) \\log(a^{2}(i)) + (1 - t(i)) \\log(1 - a^{2}(i)) \\right]$$\n",
    "\n",
    "where $t(i)$ is the true label and $a^{2}(i)$ is the predicted probability. (With the sigmoid activation function, $a^{2}(i)$ will be a value between 0 and 1.)"
   ],
   "id": "c4b5fdc937491610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def binary_crossentropy_loss(a, t):\n",
    "    \"\"\"\n",
    "    Compute binary cross-entropy loss.\n",
    "\n",
    "    Formula: L = -[t * log(a2) + (1 - t) * log(1 - a2)]\n",
    "\n",
    "    Args:\n",
    "        a: Predicted probabilities (output of sigmoid), shape (batch_size, 1) or (batch_size,)\n",
    "        t: True binary labels (0 or 1), shape (batch_size, 1) or (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar loss value\n",
    "    \"\"\"\n",
    "    # TODO: Implement binary cross-entropy loss\n",
    "    # 1. Convert inputs to numpy arrays\n",
    "    # 2. Flatten inputs if needed\n",
    "    # 3. Add epsilon for numerical stability to prevent log(0)\n",
    "    # 4. Compute the binary cross-entropy loss\n",
    "    # 5. Return the mean loss\n",
    "    \n",
    "    return None"
   ],
   "id": "e20df47edbe54d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Derivative of the Loss\n",
    "We will need to take the derivative of the loss function to begin the backpropagation process. It turns out that it is very efficient to take the direct derivative of the loss with respect to the net input $n^{2}$, rather than to take the derivative of the loss with respect to the network output $a^{2}$ and then multiply that by the derivative of the sigmoid activation function."
   ],
   "id": "9aa40ca7fa3f98c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def binary_crossentropy_logit_gradient(n, t):\n",
    "    \"\"\"\n",
    "    Compute gradient of binary cross-entropy loss with respect to the net input.\n",
    "\n",
    "    For binary cross-entropy with sigmoid:\n",
    "    L = -[t * log(sigmoid(n)) + (1 - t) * log(1 - sigmoid(n))]\n",
    "\n",
    "    The gradient w.r.t. n is simply: dL/dn = sigmoid(n) - t = a - t\n",
    "\n",
    "    Args:\n",
    "        n: net input before the sigmoid\n",
    "        t: True binary labels (0 or 1)\n",
    "\n",
    "    Returns:\n",
    "        gradient: Gradient of loss w.r.t. net input n\n",
    "    \"\"\"\n",
    "    # TODO: Implement the gradient of binary cross-entropy loss w.r.t. the net input\n",
    "    # 1. Compute sigmoid of net input\n",
    "    # 2. Compute the gradient: sigmoid(n) - t\n",
    "    # 3. Return the gradient\n",
    "    \n",
    "    return None"
   ],
   "id": "7c135def2313a647"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "Now we implement the SimpleRNN network class. The class has five methods:\n",
    "\n",
    "- `__init__` initializes the network weights and baises, and creates tensors to store the gradients.\n",
    "- `forward` takes a sequence of inputs, simulates the network, and then returns the outputs. It also saves intermediate variables in a cache for use during the backpropagation process.\n",
    "- `backward` performs the backpropagation through time algorithm to compute the gradient.\n",
    "- `clip_gradients` clips the gradients, if the norm becomes too large, to prevent exploding gradient.\n",
    "- `update_parameters` updates the weights and biases using the steepest descent algorithm.\n"
   ],
   "id": "c02436820091cc25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    Uses tanh activation for hidden state and sigmoid for output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, R: int, S1: int, S2: int, learning_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initialize the RNN with random weights.\n",
    "\n",
    "        Args:\n",
    "            R: Size of input vectors\n",
    "            S1: Number of hidden units\n",
    "            S2: Size of output vectors\n",
    "            learning_rate: Learning rate for training\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.S1 = S1\n",
    "        self.S2 = S2\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights with small random values\n",
    "        # LW_11: hidden to hidden weights\n",
    "        # IW_11: input to hidden weights\n",
    "        # LW_21: hidden to output weights\n",
    "        # b_1: hidden bias\n",
    "        # b_2: output bias\n",
    "        self.LW_11 = np.random.randn(S1, S1) * 0.1\n",
    "        self.IW_11 = np.random.randn(S1, R) * 0.1\n",
    "        self.LW_21 = np.random.randn(S2, S1) * 0.1\n",
    "        self.b_1 = np.zeros((S1, 1))\n",
    "        self.b_2 = np.zeros((S2, 1))\n",
    "\n",
    "        # Store gradients\n",
    "        self.dLW_11 = np.zeros_like(self.LW_11)\n",
    "        self.dIW_11 = np.zeros_like(self.IW_11)\n",
    "        self.dLW_21 = np.zeros_like(self.LW_21)\n",
    "        self.db_1 = np.zeros_like(self.b_1)\n",
    "        self.db_2 = np.zeros_like(self.b_2)\n",
    "\n",
    "    def forward(self, inputs: list[np.ndarray], a1_prev: np.ndarray | None = None) -> tuple[\n",
    "        list[np.ndarray], list[np.ndarray], list[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN.\n",
    "\n",
    "        Args:\n",
    "            inputs: List of input vectors (each is column vector)\n",
    "            a1_prev: Previous hidden state (if None, initialize to zeros)\n",
    "\n",
    "        Returns:\n",
    "            outputs: List of output vectors\n",
    "            a1_states: List of hidden states (including initial state)\n",
    "            cache: Dictionary containing intermediate values for backprop\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass through the RNN\n",
    "        # 1. Initialize a1_prev if None\n",
    "        # 2. Initialize empty lists for outputs, a1_states, and n2_tot\n",
    "        # 3. Loop through each input:\n",
    "        #    a. Ensure input is a column vector\n",
    "        #    b. Update hidden state: a1 = tanh(LW_11 @ a1 + IW_11 @ p + b_1)\n",
    "        #    c. Compute output: output = sigmoid(LW_21 @ a1 + b_2)\n",
    "        #    d. Store values in the appropriate lists\n",
    "        # 4. Create a cache dictionary with inputs, a1_states, and n2_tot\n",
    "        # 5. Return outputs, a1_states, and cache\n",
    "        \n",
    "        return None, None, None\n",
    "\n",
    "    def backward(self, outputs: list[np.ndarray], targets: list[np.ndarray], cache: dict) -> float:\n",
    "        \"\"\"\n",
    "        Backward pass (backpropagation through time).\n",
    "\n",
    "        Args:\n",
    "            outputs: List of output vectors from forward pass\n",
    "            targets: List of targets\n",
    "            cache: Cache from forward pass\n",
    "\n",
    "        Returns:\n",
    "            loss: Average loss over the sequence\n",
    "        \"\"\"\n",
    "        # TODO: Implement backpropagation through time (BPTT)\n",
    "        # 1. Extract values from cache\n",
    "        # 2. Get sequence length\n",
    "        # 3. Initialize gradients to zero\n",
    "        # 4. Initialize gradient for next hidden state\n",
    "        # 5. Initialize total_loss\n",
    "        # 6. Loop through the sequence in reverse order:\n",
    "        #    a. Get current values (p_t, a1_t, a1_prev, a2_t, n2_t, target_t)\n",
    "        #    b. Compute loss for current time step\n",
    "        #    c. Compute gradient of loss w.r.t. n2\n",
    "        #    d. Compute gradients for output layer\n",
    "        #    e. Compute gradient w.r.t. hidden state\n",
    "        #    f. Compute gradient through tanh activation\n",
    "        #    g. Compute gradients for hidden layer\n",
    "        #    h. Update gradient for next iteration\n",
    "        # 7. Clip gradients to prevent exploding gradients\n",
    "        # 8. Return average loss\n",
    "        \n",
    "        return 0.0\n",
    "\n",
    "    def clip_gradients(self, max_norm: float = 5.0):\n",
    "        \"\"\"Clip gradients to prevent exploding gradients.\"\"\"\n",
    "        # TODO: Implement gradient clipping\n",
    "        # 1. Create a list of all gradients\n",
    "        # 2. For each gradient:\n",
    "        #    a. Compute the norm\n",
    "        #    b. If norm > max_norm, scale the gradient\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def update_parameters(self):\n",
    "        \"\"\"Update parameters using computed gradients.\"\"\"\n",
    "        # TODO: Implement parameter updates using gradient descent\n",
    "        # 1. Update LW_11, IW_11, LW_21, b_1, and b_2 using their gradients and the learning rate\n",
    "        \n",
    "        pass"
   ],
   "id": "dc2fae0fc836d25d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the Network\n",
    "\n",
    "Now we create the training function. Its arguments are an initialized network object, a batch of input sequences, a batch of target sequences and the number of epochs of training. For each epoch, the method does a forward pass using the `forward` method, runs the `backward` method to compute the loss and the gradients and then updates the weights and biases using the `update_parameters` method. It prints the loss every 5 epochs and returns a list of losses during training."
   ],
   "id": "cf7af26a6c8be95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_rnn(rnn: SimpleRNN, sequences: list[list[np.ndarray]], targets: list[list[np.ndarray]], epochs: int = 100):\n",
    "    \"\"\"Train the RNN on the given dataset.\"\"\"\n",
    "    # TODO: Implement the training loop for the RNN\n",
    "    # 1. Initialize an empty list for losses\n",
    "    # 2. Loop through epochs:\n",
    "    #    a. Initialize total_loss\n",
    "    #    b. Loop through each sequence and target:\n",
    "    #       i. Perform forward pass\n",
    "    #       ii. Perform backward pass and get loss\n",
    "    #       iii. Update parameters\n",
    "    #       iv. Add loss to total_loss\n",
    "    #    c. Compute average loss\n",
    "    #    d. Append average loss to losses list\n",
    "    #    e. Print loss every 5 epochs\n",
    "    # 3. Return losses\n",
    "    \n",
    "    return []"
   ],
   "id": "33a2222bced0ea66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test Run\n",
    "\n",
    "Now let's test the RNN by creating a training dataset with `generate_counting_ones_data` and training the network. First create the dataset and the network. We will start by using a window size of 3, so the network will be looking back three time steps to see if there are an even number of ones. Create 1000 sequences of length 10."
   ],
   "id": "d008e99c20b84679"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_samples = 1000  # Number of training examples\n",
    "seq_length = 10     # Length of each sequence\n",
    "window_size = 3     # Look back 3 steps\n",
    "\n",
    "X, y = generate_counting_ones_data(num_samples, seq_length, window_size)\n",
    "\n",
    "X = np.expand_dims(X, axis=2)\n",
    "y = np.expand_dims(y, axis=2)\n",
    "\n",
    "# Create RNN\n",
    "rnn = SimpleRNN(R=1, S1=20, S2=1, learning_rate=0.1)"
   ],
   "id": "8e9d0603a14fde8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now train the network for 10 epochs.",
   "id": "a0d6a4e879b6bf13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the RNN\n",
    "# TODO: Uncomment the following line after implementing the train_rnn function\n",
    "# losses = train_rnn(rnn, X, y, epochs=10)"
   ],
   "id": "957caf3737be1838"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's plot the loss function during training to see how the network has converged.",
   "id": "4b79e60eb7524085"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot training curve\n",
    "# TODO: Uncomment the following code after implementing and running the training\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(losses)\n",
    "# plt.title('RNN Training Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.yscale('log')\n",
    "# plt.grid(True)"
   ],
   "id": "ed2006ffbceb0bc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test the Trained Network\n",
    "\n",
    "The loss trajectory should show that the loss is still decreasing after 10 epochs. We'll check the network by giving it a binary input. Let's see if the network output is truly indicating at each time step whether there have been an even or an odd number of ones in the window. An even number should produce a 1, and an odd number should produce a 0. Compare the input and output sequences to verify that this is true."
   ],
   "id": "f33e61d5a0782108"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Make a numpy array test input of zeros and ones.\n",
    "test_input = np.array([1,0,1,1,0,0,0,0,1,0,1,0,0,0,0,1,1,1,1,0,0,0])\n",
    "\n",
    "# The batch size is 1, so we add a dimension\n",
    "test_input = np.expand_dims(test_input, axis=1)\n",
    "\n",
    "# Pass the input through the network\n",
    "# TODO: Uncomment the following line after implementing the forward method\n",
    "# outputs, _, _ = rnn.forward(test_input)\n",
    "\n",
    "# Print the inputs and outputs\n",
    "print('Inputs:')\n",
    "print(test_input.transpose(1,0))\n",
    "# TODO: Uncomment the following lines after implementing the forward method\n",
    "# print('Outputs:')\n",
    "# print([f\"{ii[0][0]:.4e}\" for ii in outputs])\n",
    "# print('Outputs rounded to nearest integer:')\n",
    "# print([np.int64(np.round(ii[0][0])) for ii in outputs])"
   ],
   "id": "b3cf59cca0dffb08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyTorch Implementation\n",
    "\n",
    "Now implement the same network using PyTorch. We need to first import the PyTorch libraries"
   ],
   "id": "229cfc5cee61c81c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import the needed PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "id": "73fbc28dd62db693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing the RNN in PyTorch\n",
    "\n",
    "We now create the network class. We don't need to define the sigmoid activation function, the binary cross-entropy loss or its derivative, because they are built in to PyTorch. Also, we don't need to store layer outputs at each time step, or have a backward method, because PyTorch does this in the background. PyTorch will unfold the network and perform the backpropagation automatically. The network class has these methods:\n",
    "\n",
    "- `__init__` defines the network layers, loss function and optimizer.\n",
    "- `forward` takes a sequence of inputs, simulates the network, and then returns the outputs.\n",
    "- `compute_loss` computes the loss for a batch of data.\n",
    "- `train_step` performs the standard PyTorch training sequence - zeros the gradient, passes forward through the network, computes the loss, passes backward to compute the gradient, clips the gradient and updates the weights."
   ],
   "id": "40d98fed8f2381d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimplePyTorchRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, R: int, S1: int, S2: int, learning_rate: float = 0.01):\n",
    "        super(SimplePyTorchRNN, self).__init__()\n",
    "\n",
    "        self.R = R\n",
    "        self.S1 = S1\n",
    "        self.S2 = S2\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Define layers -\n",
    "        self.IW_11 = nn.Linear(R, S1, bias=True)\n",
    "        self.LW_11 = nn.Linear(S1, S1, bias=False)\n",
    "        self.LW_21 = nn.Linear(S1, S2, bias=True)\n",
    "\n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()  # Note: we'll use BCE loss with sigmoid\n",
    "\n",
    "        # Loss function - Binary Cross Entropy\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Optimizer - compare with manual parameter updates\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, a1_prev: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN.\n",
    "\n",
    "        Compare with numpy version:\n",
    "        - No manual cache management needed\n",
    "        - PyTorch tracks computation graph automatically\n",
    "        - Batch processing is built-in\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_len, R)\n",
    "            a1_prev: Previous hidden state (batch_size, S1)\n",
    "\n",
    "        Returns:\n",
    "            outputs: Output probabilities (batch_size, seq_len, S2)\n",
    "            a1_tot: All hidden states (batch_size, seq_len+1, S1)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass for the PyTorch RNN\n",
    "        # 1. Get batch_size and seq_len from inputs\n",
    "        # 2. Initialize a1_prev if None\n",
    "        # 3. Initialize empty lists for outputs and a1_tot\n",
    "        # 4. Loop through each time step:\n",
    "        #    a. Update hidden state using tanh activation\n",
    "        #    b. Compute output using sigmoid activation\n",
    "        #    c. Store values in the appropriate lists\n",
    "        # 5. Stack outputs and hidden states\n",
    "        # 6. Return outputs and a1_tot\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "    def compute_loss(self, outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute loss using PyTorch's built-in functions.\n",
    "\n",
    "        Compare with numpy version:\n",
    "        - No manual cross-entropy implementation\n",
    "        - No numerical stability concerns\n",
    "        - Automatic broadcasting\n",
    "        \"\"\"\n",
    "        # TODO: Implement the loss computation for the PyTorch RNN\n",
    "        # 1. Reshape outputs and targets for the loss function\n",
    "        # 2. Return the computed loss\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def train_step(self, inputs: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "\n",
    "        Compare with numpy version:\n",
    "        - No manual gradient computation (backward())\n",
    "        - No manual parameter updates (optimizer.step())\n",
    "        - No gradient clipping needed (can be added easily)\n",
    "        \"\"\"\n",
    "        # TODO: Implement a single training step for the PyTorch RNN\n",
    "        # 1. Zero gradients\n",
    "        # 2. Perform forward pass\n",
    "        # 3. Compute loss\n",
    "        # 4. Perform backward pass\n",
    "        # 5. Clip gradients (optional)\n",
    "        # 6. Update parameters\n",
    "        # 7. Return loss value\n",
    "        \n",
    "        return 0.0"
   ],
   "id": "da764c6d862b3d04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Collating the Sequence\n",
    "\n",
    "Because PyTorch can use a data loader and can train using minibatches, we create a method that will collate sequences and can have sequences of different length within the same minibatch."
   ],
   "id": "f6429eacb99c8a21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def collate_sequences(sequences: list[list[np.ndarray]], targets: list[list[int]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Convert variable-length sequences to padded tensors for batch processing.\n",
    "\n",
    "    This shows how PyTorch handles variable-length sequences compared to\n",
    "    the loop-based approach in the numpy version.\n",
    "    \"\"\"\n",
    "    # TODO: Implement sequence collation for PyTorch\n",
    "    # 1. Find maximum sequence length\n",
    "    # 2. Initialize empty lists for padded sequences and targets\n",
    "    # 3. Loop through each sequence and target:\n",
    "    #    a. Convert sequence to tensor\n",
    "    #    b. Pad sequence if needed\n",
    "    #    c. Convert target to tensor\n",
    "    #    d. Pad target if needed\n",
    "    #    e. Append to the appropriate lists\n",
    "    # 4. Stack padded sequences and targets\n",
    "    # 5. Return stacked tensors\n",
    "    \n",
    "    return None, None"
   ],
   "id": "c779f0e3d5763d95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Function\n",
    "\n",
    "Now we create the training function. It takes as arguments an initialized network, the input sequences (a list of lists of numpy arrays), the targets (a list of lists of integers), and the number of epochs. It trains the network and then returns a list of losses at each epoch."
   ],
   "id": "f7df060ad88c5c1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_pytorch_rnn(rnn: SimplePyTorchRNN, sequences: list[list[np.ndarray]], targets: list[list[int]], epochs: int = 100,\n",
    "                      batch_size: int = 32):\n",
    "    \"\"\"\n",
    "    Train the PyTorch RNN with batch processing.\n",
    "\n",
    "    Compare with numpy version:\n",
    "    - Batch processing built-in\n",
    "    - Automatic gradient computation\n",
    "    - No manual parameter updates\n",
    "    - Much shorter code\n",
    "    \"\"\"\n",
    "    # TODO: Implement the training loop for the PyTorch RNN\n",
    "    # 1. Convert sequences and targets to tensors using collate_sequences\n",
    "    # 2. Create a dataset and dataloader\n",
    "    # 3. Initialize an empty list for losses\n",
    "    # 4. Loop through epochs:\n",
    "    #    a. Initialize epoch_loss and num_batches\n",
    "    #    b. Loop through each batch:\n",
    "    #       i. Perform a training step and get loss\n",
    "    #       ii. Add loss to epoch_loss\n",
    "    #       iii. Increment num_batches\n",
    "    #    c. Compute average loss\n",
    "    #    d. Append average loss to losses list\n",
    "    #    e. Print loss every 10 epochs\n",
    "    # 5. Return losses\n",
    "    \n",
    "    return []"
   ],
   "id": "8b8d679f2bd8468a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test Run\n",
    "\n",
    "Now let's test the RNN by creating a training dataset with `generate_counting_ones_data` and training the network. First create the dataset and the network. As in the Python/Numpy version, we will start by using a window size of 3, so the network will be looking back three time steps to see if there are an even number of ones. Generate 1000 sequences of length 10."
   ],
   "id": "53eb2cf47f1da92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_samples = 1000  # Number of training examples\n",
    "seq_length = 10     # Length of each sequence\n",
    "window_size = 3     # Look back 3 steps\n",
    "\n",
    "X, y = generate_counting_ones_data(num_samples, seq_length, window_size)\n",
    "\n",
    "X = np.expand_dims(X, axis=2)\n",
    "y = np.expand_dims(y, axis=2)\n",
    "\n",
    "rnn = SimplePyTorchRNN(R=1, S1=20, S2=1, learning_rate=0.1)"
   ],
   "id": "7cfdc469bead8fd9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now train the network for 10 epochs.",
   "id": "519f6e584f6fd9cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Uncomment the following line after implementing the train_pytorch_rnn function\n",
    "# losses = train_pytorch_rnn(rnn, X, y, epochs=10, batch_size=16)"
   ],
   "id": "81a5a89fbe3e0f2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now plot the loss function during training to see how the network has converged.",
   "id": "b01a7590db6f1750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot training curve\n",
    "# TODO: Uncomment the following code after implementing and running the training\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(losses)\n",
    "# plt.title('PyTorch RNN Training Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.yscale('log')\n",
    "# plt.grid(True)"
   ],
   "id": "67d14e7efa80b883"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test the Trained Network\n",
    "\n",
    "We'll perform the same test that we used for the Python/Numpy version. The results should be similar, since both programs are performing gradient descent on the same RNN."
   ],
   "id": "df3f9a628c677c27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_input = torch.tensor(np.array([1,0,1,1,0,0,0,0,1,0,1,0,0,0,0,1,1,1,1,0,0,0]))\n",
    "test_input = test_input.to(torch.float32)\n",
    "test_input = torch.unsqueeze(test_input, 0)\n",
    "test_input = torch.unsqueeze(test_input, 2)\n",
    "\n",
    "# TODO: Uncomment the following line after implementing the forward method\n",
    "# outputs, _ = rnn.forward(test_input)\n",
    "\n",
    "# Print the inputs and outputs\n",
    "print('Inputs:')\n",
    "print(test_input[0,:,0].long())\n",
    "# TODO: Uncomment the following lines after implementing the forward method\n",
    "# print('Outputs:')\n",
    "# print(outputs[0,:,0])\n",
    "# print('Outputs rounded to nearest integer:')\n",
    "# print(torch.round(outputs[0,:,0]).long())"
   ],
   "id": "d2cb463760c4d5e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Explore Further\n",
    "\n",
    "Experiment with different window sizes in the training data. The longer the window, the more difficult it will be to train the network, because the network will have to increase the length of its memory. Some other things you could experiment with are:\n",
    "- other optimizers, like Adam.\n",
    "- learning rates.\n",
    "- numbers of neurons in the hidden layer, $S_{1}$.\n",
    "- different test sequences.\n",
    "- numbers of sequences in the training set.\n",
    "- length of training set sequences.\n",
    "- modify the code to use an LSTM, instead of an RNN. Compare the abilities of each to handle longer window sizes.\n",
    "- Could this problem be solved with a FTDNN? What is an advantage of using an RNN instead?"
   ],
   "id": "4b86a1da9b0708cc"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
