{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NNDesignDeepLearning/NNDesignDeepLearning/blob/master/10.PyTorchIntroChapter/Code/LabSolutions/PyTorchIntroLab1_Solution.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Introduction Lab 3 -- Transfer Learning\n",
    "\n",
    "In the first PyTorch lab, you defined and trained a convolution network from scratch, with random initial weights. In this lab you will use transfer learning, in which you start with a network that has already been trained on a large data set and then fine-tune it on your data. The idea is that the pre-trained network will have already learned some features that will be relevant to your data. This is especially useful when your dataset is small. Since some features have already been learned, the fine-tuning process does not have to be as long as training from scratch, and so there is less chance of overfitting.\n",
    "\n",
    "When using a pretrained network, we need to do some preprocessing of our data to match some of the characteristics of the data that was used to pre-train the model. The most commonly used dataset for model pre-training is the [Imagenet](https://image-net.org/) dataset. We will change the shape of the input images to match Imagenet images.\n",
    "\n",
    "Also, we will need to modify the final layers of the pretrained network so that the output dimension matches with the number of classes associated with our dataset.\n",
    "\n",
    "Some of the cells in this notebook are prefilled with working code. In addition, there will be cells with missing code (labeled `# TODO`), which you will need to complete. If you need additional cells, you can use the `Insert` menu at the top of the page.\n",
    "\n",
    "## Loading Modules\n",
    "\n",
    "We begin by loading some useful modules. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.models import alexnet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "For this lab we will use a dataset of photos of cats and dogs that are labeled according to breed. The original website for the data can be found [here](https://www.robots.ox.ac.uk/~vgg/data/pets/). The data set can be accessed easily using `torchvision.datasets` as `OxfordIIITPet`. When loading the dataset you are going to perform several transforms on the input and one transform on the target.\n",
    "\n",
    "The images in the dataset are assigned to 37 breed categories -- 12 cat breeds and 25 dog breeds. You will use a custom target transformation, `modify_label()`, which will convert the target labels to 0 for cat and 1 for dog."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert to cat/dog categories\n",
    "def modify_label(label):\n",
    "    cats = [0, 5, 6, 7, 9, 11, 20, 23, 26, 27, 32, 33]\n",
    "    # Cats\n",
    "    if label in cats:\n",
    "        return 0\n",
    "    # The rest are dogs\n",
    "    else:\n",
    "        return 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next define some transforms to perform on the input images. Make a dictionary with keys 'train' and 'test', because you will use different transformations for the two datasets. For each dictionary entry, use `transforms.Compose()` to combine a list of transformations.\n",
    "\n",
    "The first input transformation is `Resize((224, 224))` to resize the images to 224xx224. The data set has images of a variety of sizes, but the neural network that you are going to use expects all inputs to be 224x224. The final input transformation is `ToTensor()`, which we used in the first lab. For the test set these will be the only transformations. For the training set you can add additional transformations, such as `RandomHorizontalFlip()`, `RandomRotation(10)` and `ColorJitter()` for data augmentation. Since the dataset is fairly small, the results will be more robust if we add additional reasonable images."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.,   # TODO\n",
    "        transforms.,   # TODO\n",
    "        transforms.,   # TODO\n",
    "        transforms.,   # TODO\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.,   # TODO\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that the transorms have been defined, we can load the dataset. For the training data, use `data_transforms['train']` for the `transform`, and use `modify_label` for the `target_transform`. For the test data, use `data_transforms['test']` for the `transform`, and use `modify_label` for the `target_transform`."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO\n",
    "training_data = datasets.OxfordIIITPet(\n",
    "    root='data',\n",
    "    split='trainval',\n",
    "    target_types=,\n",
    "    download=,\n",
    "    transform= ,\n",
    "    target_transform=\n",
    ")\n",
    "# TODO\n",
    "test_data = datasets.OxfordIIITPet(\n",
    "    root='data',\n",
    "    split='test',\n",
    "    target_types=,\n",
    "    download=,\n",
    "    transform=,\n",
    "    target_transform=\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the next cell, print out the number of examples in the training and test sets, the shape of the first feature and the first label."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Length of Training Data\n",
    "# TODO\n",
    "print()\n",
    "# Length of Testing Data\n",
    "# TODO\n",
    "print()\n",
    "# The shape of the first feature\n",
    "# TODO\n",
    "print()\n",
    "# The first label\n",
    "# TODO\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now plot the first image and label in the training set to see if they match. For cats, the label is 0, for dogs the label is 1. Note that we have to permute the dimensions to be suitable for `plt.imshow()`."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# First training label\n",
    "# TODO\n",
    "print()\n",
    "# First training image\n",
    "plt.imshow(torch.permute(training_data[0][0], (1, 2, 0)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Put the training and testing data into DataLoaders. Use a batch size of 100 for both sets, and shuffle the training data, but not the test data."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO\n",
    "BATCH_SIZE =\n",
    "train_loader = DataLoader()\n",
    "test_loader = DataLoader()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Model\n",
    "\n",
    "Now that the data is loaded, the next step is to construct the model. For this lab you want to begin with a model that has already been trained on Imagenet. We are going to use [alexnet](https://pytorch.org/vision/master/models/alexnet.html). A list of other models that are loaded in PyTorch can be found [here](https://pytorch.org/vision/0.9/models.html). Load the model with the Imagenet weights by using the option `weights='DEFAULT'`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "model ="
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When using transfer learning there are usually two options. In the first option, only the weights in the final classifier layer are updated during training. In the second option, all the weights in the network are updated. We will start with option 1. This has the least chance of overfitting, since a small number of weights are being adjusted.\n",
    "\n",
    "In the next cell we are going to turn off training of the network weights. Later we will modify the last layer of the network and turn training on for that layer only. To turn training off for a given set of parameters, you set `params.requires_grad` to `False`. All the model parameters are contained in the iterable `model.parameters()`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for params in model.parameters():\n",
    "    # TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print out the model to see: the network architecture, the label of the final classifier layer and the number of output features."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(model)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, the final layer of the classifier is `model.classifier[6]`. It has 1000 output features. Since we only have two categories (0 for cat and 1 for dog), we need to change `model.classifier[6].out_features` to 2."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will turn on training for that last `Linear` layer."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for params in model.classifier[6].parameters():\n",
    "    # TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "After constructing the model, print a final summary to verify that we have 2 output features."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(model)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network\n",
    "\n",
    "The remainder of the training process is the same as it was in the first lab. The first step in training the network is to select the optimizer. Use `Adam` as the training function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO\n",
    "optimizer ="
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Assign the loss function as `nn.CrossEntropyLoss()`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "loss_fn ="
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Write a training loop. First, use a GPU if one is available. Train for 10 epochs, using the train_loader created above. Every 100 iterations, print out the training loss for the current minibatch, and save the loss for later plotting. (This is the same training loop from the first pytorch lab.)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "total_loss =  []\n",
    "ind =  []\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            total_loss.append(loss.item())\n",
    "            ind.append(batch_idx + epoch*len(train_loader)/BATCH_SIZE)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Plot the loss that you saved in the training loop."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(total_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Trained Model\n",
    "\n",
    "In a loop of minibatches, using the test_loader, compute the overall accuracy of the network on the test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Testing loop\n",
    "correct = 0\n",
    "mistakes = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To check that you are not overfitting, calculate the accuracy on the training data. If the network was well trained, the accuracies should be similar."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in train_loader:\n",
    "        # TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\nTrain set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    correct, len(train_loader.dataset),\n",
    "    100. * correct / len(train_loader.dataset)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Further\n",
    "\n",
    "When using transfer learning there are a number of options available. Here are a few things you can try.\n",
    "\n",
    "1. After training only the weights in the final layer, fine-tune the network by adjusting all the weights at the same time. What advantages do you find? What disadvantages? Be sure to compare training and testing performance. Do you think you should use a different learning rate when fine-tuning?\n",
    "1. The network was initialized with weights learned from the Imagenet dataset. You can also start with random weights by leaving out the `weights` parameter. Try training the random network from scratch, first by only training the last layer and then by training all the weights. How do your results differ from when you used the pretrained network?\n",
    "1. There are many other networks that can be loaded from `torchvision.models`, which you can find [here](https://pytorch.org/vision/0.9/models.html). Try some other networks and compare your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
