{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NNDesignDeepLearning/NNDesignDeepLearning/blob/master/10.PyTorchIntroChapter/Code/ChapterNotebook/PyTorchIntroChapter.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "This chapter provides an introduction to the **PyTorch** framework for deep learning. At the moment, **TensorFlow** and **PyTorch** are the two most popular frameworks. Over time these two frameworks have become more similar, as they borrow each other's best features. As in Chapter 6, we will show how you can quickly get started to perform all aspects of the typical deep learning workflow: load and preprocess data, build a neural network, train the network and evaluate the results.\n",
    "\n",
    "\n",
    "# Theory and Examples\n",
    "\n",
    "\n",
    "The history of PyTorch begins around 2002 with Torch, an open-source machine learning library, scientific computing framework, and scripting language based on Lua. It was originally developed at the Idiap Research Institute at EPFL in Lausanne, Switzerland. The main developers were Ronan Collobert, Samy Bengio and Johnny Mari√©tho. Torch evolved over multiple revisions through version 7 in 2017.\n",
    "\n",
    "The front-end for Torch was then converted by Soumith Chintala, Adam Paszke, Sam Gross and Gregory Chanan at Facebook (now Meta) from Lua to Python, at which time it was renamed PyTorch. It is written in Python, C++ and CUDA. PyTorch 2.0 was released on 15 March 2023 and, at the time of this writing, is currently governed by the PyTorch Foundation, a subsidiary of the Linux Foundation.\n",
    "\n",
    "TensorFlow and PyTorch are very similar, and the deep learning workflow (load and process data, build a neural network, train the network and evaluate the results) is the same. Therefore, this chapter will follow the pattern of Chapter 6, but we will emphasize areas where there are differences between the two frameworks.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "PyTorch has the same formats for tensors as TensorFlow. Refer to Chapter 6 to review how features, samples, timesteps and channels are indexed in a tensor. PyTorch tensors can be created directly from data, using the `torch.tensor()` command, or from a numpy array, using the `torch.from_numpy()` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR \n",
    "As in the TensorFlow chapter, we begin by generating data for the XOR problem. For this example we use `torch.tensor()` to create the tensor. We do not need to change the targets using a `to_categorical()` operation, because PyTorch will take care of that when we use the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:56:44.801401Z",
     "start_time": "2025-04-01T19:56:43.464842Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "p = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]],dtype=torch.float32)\n",
    "t = torch.tensor([0, 1, 1, 0])\n",
    "print(p)\n",
    "print(t)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "In TensorFlow, the core data structure was the **model**, based on `tensorflow.keras.Model`. The equivalent PyTorch **module** class is based on `torch.nn.Module` The model is a way to organize layers of a network. We will describe two techniques, which have equivalent methods in TensorFlow, to define a model: 1) the sequential class and 2) the module subclass. (In the labs we will consider a third method: using a **pretrained model**.)\n",
    "\n",
    "For the sequential class, you use the sequential container `torch.nn.Sequential`. Modules are added in the order they are passed to the constructor, as in the following example. Here we create a two-layer network with 10 neurons in the hidden layer and a `tanh` activation function. The network architecture is 2-10-2. (Note that, unlike in Chapter 6, we are not using the softmax activation on the last layer, because it is included in the PyTorch `CrossEntropyLoss`.)\n",
    "\n",
    "The sequential class is designed for networks where each layer follows the previous one. The `Linear` layer is standard matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 10),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(10, 2)\n",
    ")\n",
    "print(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "When you use the **module subclass** method, you create your own fully-customizable models by subclassing the `Model` class and implementing your own forward pass in the `forward` method. The following code implements the same 2-10-2 network using the model subclass method."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TwoLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = torch.nn.Linear(2, 10)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.dense2 = torch.nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "model = TwoLayer()\n",
    "print(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "After the data has been loaded and the network has been created, the next step is to train the network. In the following we will cover the basic training steps.\n",
    "\n",
    "Before training a network, we need to select the optimizer to be used. In the following example we use the Adam optimizer. The first argument of the optimizer contains the variables to be adjusted, which are contained in `model.parameters()`. When setting the optimizer, you can also specify optimizer-specific options such as the learning rate, momentum, etc."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also need to define the loss function. Here we will use the cross entropy loss."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "loss_fn = torch.nn.CrossEntropyLoss()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train the network. In TensorFlow, we used the `fit` method, but in PyTorch we perform the following steps for each iteration:\n",
    "\n",
    "1. Zero the gradient\n",
    "2. Make a forward pass through the network\n",
    "3. Calculate the loss\n",
    "4. Compute the gradient\n",
    "5. Update the weights\n",
    "\n",
    "The following loop illustrates the process. In this case we pass the entire data set through the network in each iteration. In the labs we will use minibatches. Also, in a difference from TensorFlow, we check to see if a GPU is available, and if so we move the model and the data to the GPU. (If using minibatches, a minibatch would be moved to the GPU at each iteration.) For this simple problem, using the GPU is actually slower than using the cpu."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Check if a GPU is available, and move model and data\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "p = p.to(device)\n",
    "t = t.to(device)\n",
    "\n",
    "epochs = 5000\n",
    "loss_values = []\n",
    "for epoch in range(epochs):\n",
    "    # Zero the gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass through the network\n",
    "    output = model(p)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(output, t)\n",
    "\n",
    "    # Compute the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # Save the loss values for plotting\n",
    "    loss_values.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We saved the loss values at each iteration, so that we could view the process."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(range(epochs), loss_values)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "To calculate the network response for any input you can invoke the model: `model(p)`. Here we apply the training inputs to the model to check the response. (We manually add the `Softmax` function, since it wasn't included in the model. The argument `dim` is the dimension along which the softmax outputs will sum to one.) Is the response correct for the **XOR** problem?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n = model(p)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "a = softmax(n)\n",
    "print(a)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Loading\n",
    "\n",
    "As described in the TensrFlow chapter, the ETL process of loading/formatting/pre-processing data is one of the most important parts of the deep learning workflow. The procedures for doing this in PyTorch are very similar to those in TensorFlow.\n",
    "\n",
    "TensorFlow had the `tf.data.Dataset` API for creating input pipelines. PyTorch has `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`. They enable you to use pre-loaded datasets or your own data. `Dataset` defines what the data are and how to access them, while `DataLoader` defines how to load that data efficiently for model training or inference.\n",
    "\n",
    "PyTorch `Dataset`:\n",
    "\n",
    "1. Represents the data and enables access to individual samples\n",
    "2. Each item of the dataset has an input and a target\n",
    "3. Defines how to load and preprocess individual data points\n",
    "4. Implements `__getitem__()` to retrieve a single sample and `__len__()` for the total number of samples\n",
    "5. Focuses on data representation and access\n",
    "\n",
    "PyTorch `DataLoader`:\n",
    "\n",
    "1. Wraps a `Dataset` and provides utilities for batching, shuffling, and parallel data loading\n",
    "2. Handles the iteration over the dataset, creating batches, and optionally shuffling\n",
    "3. Allows easy specification of batch size, number of worker processes, etc.\n",
    "4. Focuses on efficiently feeding data to the model during training/evaluation\n",
    "\n",
    "\n",
    "To illustrate the operations of `Dataset` and `DataLoader`, we'll work with the CSV file that we used in the Python chapter and lab and the TensorFlow chapter. First, we read in the CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "data_path = '../../../05.PythonChapter/Code/ChapterNotebook/'\n",
    "sample_df = pd.read_csv(data_path + 'SampleDF.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract two columns that we will use as inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "P = np.array(sample_df['FVC'])\n",
    "T = np.array(sample_df['Percent'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's define a `Dataset` class. It should have `__init__()`, `__getitem__()` and `__len__()` methods. The example below is the simplest possible `Dataset`. It loads in the features and targets, it determines the length of the dataset, and it retrieves one item, which consists of a feature and a target. There are many data handling/preprocessing tasks that could also be performed in a `Dataset`: reading data from files, parsing structured data formats (e.g., JSON, XML), normalization, augmentation, etc. We will consider some of these in the labs."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, P, T):\n",
    "        # convert into PyTorch tensors\n",
    "        self.P = torch.tensor(P, dtype=torch.float32)\n",
    "        self.T = torch.tensor(T, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # this should return the size of the dataset\n",
    "        return len(self.P)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return one input and one target sample from the dataset\n",
    "        features = self.P[idx]\n",
    "        target = self.T[idx]\n",
    "        return features, target"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now we load the data into the `Dataset`."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "dataset = SimpleDataset(P, T)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have the data loaded into the `Dataset`, we can wrap the data in a `DataLoader`. This enables us to control how the data will be fed to the training process (batch size, shuffling)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "loader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=4)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The `DataLoader` is an iterable, like a data generator, so we can access the elements within the training `for` loop."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for feat, targ in loader:\n",
    "  print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "  break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Here you can see that one minibatch of four inputs and targets has been retrieved, since we set the `batch_size` to 4."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will investigate other options of `Dataset` and `DataLoader` that allow us to distribute operations across multiple gpus, incorporate augmentation into the data pipeline, prefetch data, etc. in the labs and case studies.\n",
    "\n",
    "# Epilogue\n",
    "\n",
    "PyTorch and TensorFlow are currently the most popular deep learning frameworks. They have many similarities, and their performance in terms of computation, training times and memory storage are comparable. They both have strong community support, good documentation and active development. PyTorch has a more intuitive API, and enables faster development and easier debugging. TensorFlow has more deployment options, stronger mobile support and is more established in industry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
