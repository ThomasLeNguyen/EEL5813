{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NNDesignDeepLearning/NNDesignDeepLearning/blob/master/05.PythonChapter/Code/Labs/PythonLab1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Lab 1 Objective\n",
    "\n",
    "This objective of this first Python lab is to help you become familiar with common Python operations that are often used in deep learning workflows. If you haven't already done so, run the cells in the `PythonChapter.ipynb` Jupyter Notebook to prepare for this lab.\n",
    "\n",
    "Some of the cells in this notebook are prefilled with working code. In addition, there will be cells with missing code (labeled `# TODO`), which you will need to complete. If you need additional cells, you can use the `Insert` menu at the top of the page.\n",
    "\n",
    "## Loading Modules\n",
    "\n",
    "We begin by loading some useful modules.  In addition to NumPy and Pandas, we use the popular Python package, [Matplotlib](https://matplotlib.org/), to perform plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduler\n",
    "\n",
    "When training neural networks with gradient descent, it is important to set the learning rate appropriately. If it is too small, the training will be slow. If is is too large, the training can become unstable. In addition, the best learning rate can change during the training. In the following cells we will define functions that can compute different learning rates at different epochs of training.\n",
    "\n",
    "To begin, we define a simple algorithm that starts with a large learning rate and then reduces the rate during the training process by a certain factor at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_reduce(epoch):\n",
    "    lr_decay = 0.8\n",
    "    lr_max = 0.005\n",
    "    lr_min = 0.001\n",
    "    lr = (lr_max - lr_min)*lr_decay**epoch + lr_min\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this function, we calculate the learning rate up to an epoch of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(20), [lr_reduce(ep) for ep in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the idea of the `lr_reduce` function, create a function that starts at a low learning rate `lr_0`, increases linearly for `lr_0_steps` to a maximum of `lr_max`, holds at `lr_max` for `lr_max_steps` and then decays geometrically at a rate of `lr_decay` each epoch toward a minimum of `lr_min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_increase_decrease(epoch):\n",
    "    lr_0         = 0.001\n",
    "    lr_0_steps   = 5\n",
    "    lr_max       = 0.005\n",
    "    lr_max_steps = 5\n",
    "    lr_min       = 0.0005\n",
    "    lr_decay     = 0.8\n",
    "\n",
    "    if epoch < lr_0_steps:\n",
    "        lr = #TODO\n",
    "    elif epoch < lr_0_steps + lr_max_steps:\n",
    "        lr = #TODO\n",
    "    else:\n",
    "        lr = #TODO\n",
    "        \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(30), [lr_increase_decrease(ep) for ep in range(30)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to create learning rate functions with different sets of parameters and to pass these functions to a training function. Using the ideas above, create a `Class` of *increase-then-reduce* learning rate functions. It should have `__init__` and `__call__` methods. The input to the `__call__` method should be epoch number, and the returned value should be the learning rate for that epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_inc_dec:\n",
    "    def __init__(self, lr_0=0.001, lr_0_steps=5, lr_max = 0.005, lr_max_steps = 5, lr_min=0.0005, lr_decay=0.9):\n",
    "        self.lr_0         = lr_0\n",
    "        self.lr_max       = lr_max\n",
    "        self.lr_0_steps   = lr_0_steps\n",
    "        self.lr_max_steps = lr_max_steps\n",
    "        self.lr_min       = lr_min\n",
    "        self.lr_decay     = lr_decay\n",
    "        \n",
    "    def __call__(self, epoch):\n",
    "        #TODO\n",
    " \n",
    "        return self.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = lr_inc_dec(lr_0=0.002, lr_0_steps=10, lr_max = 0.01, lr_max_steps = 5, lr_min=0.001, lr_decay=0.85)\n",
    "plt.plot(np.arange(50), [lr_scheduler(ep) for ep in range(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Simple Network\n",
    "\n",
    "Consider the network and training data from Exercise E3.1. You are going to write some code to create the network and the training data, and then to train the network using steepest descent.\n",
    "\n",
    "## Training Data\n",
    "\n",
    "First, create NumPy arrays to hold the inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = #TODO\n",
    "T = #TODO\n",
    "print(P)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Network\n",
    "\n",
    "The next step is to define the network. Define a network class, `simplenet`, to implement the network from Exercise E2.1. The `simplenet` object should have two attributes: the weight `.w` and the bias `.b`, which should be initialized in the `__init__` method. It should have a method called `sim`, which should return the network output for a given input (and should work for a single input, or a batch of inputs). It should also have a method `deriv`, which should return the derivative of the network output with respect to the weight and bias. The derivative will have two rows (the first for w and the second for b), and as many columns as the batch size. (Hint: `np.ones` and `np.vstack` could be useful in forming the derivative.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simplenet:\n",
    "    def __init__(self, weight, bias):\n",
    "        self.w = weight\n",
    "        self.b = bias\n",
    "        \n",
    "    def sim(self, p):\n",
    "        return #TODO\n",
    "    \n",
    "    def deriv(self, p):\n",
    "        return #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the network using an initial weight of 1 and a bias of 0. Use the training data as input. The network output should be [-1,  0,  1]. The derivative should be [[-1.,  0.,  1.], [ 1.,  1.,  1.]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simplenet(1, 0)\n",
    "A = net.sim(P)\n",
    "print(A)\n",
    "dA_dwb = net.deriv(P)\n",
    "print(dA_dwb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Function\n",
    "\n",
    "Now we define the performance (or loss) function. Define a mean square error performance class. The class should have two methods: `value`, which returns the value of the MSE, and `deriv`, which returns the derivative of the MSE with respect to the network output. The input to both methods should be the network output and the target output. The function should work for single examples, or a batch of any size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mse:\n",
    "    def value(self, a, t):\n",
    "       perf = #TODO\n",
    "       return perf\n",
    "    \n",
    "    def deriv(self, a, t):\n",
    "        df_da = #TODO\n",
    "        return df_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function using the training targets and the network output you computed above. The MSE value should be 0.9166666, and the derivative should be [-1.,  1.,  3.]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = mse()\n",
    "performance = test_mse.value(A, T)\n",
    "print(performance)\n",
    "dF_dA = test_mse.deriv(A, T)\n",
    "print(dF_dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "Now define a training function. There should be five inputs to the training function: 1) a network, 2) a performance function, 3) a learning rate scheduler, 4) the maximum number of epochs to train, and 5) the data set. The data set should be provided as a dictionary with two keys: `Input` and `Target`. The values in the dictionary should be NumPy arrays containing the training inputs and training targets.  \n",
    "\n",
    "To compute the gradient, use the `perf.deriv`  and `net.deriv` methods and then multiply their outputs together ($\\partial F/\\partial \\mathbf{x}=\\partial F/\\partial a\\times \\partial a/\\partial \\mathbf{x}$). Get the learning rate at each epoch from the learning rate scheduler. This will be a batch traning function, which will use the entire data set for each weight update, so each iteration is one epoch. (See Chapter 3 of the text for a discussion of batch training.) You can access the weight and bias attributes of the network with `net.w` and `net.b`.\n",
    "\n",
    "Compute the performance at each epoch and store it in a list (use the `append` method). The training function should return two things: 1) the final trained network, and 2) the list of performance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gd(net, perf, lr_sched, max_epoch, data):\n",
    "    pp = data['Input']\n",
    "    tt = data['Target']\n",
    "    ff = []\n",
    "    for ep in range(max_epoch):\n",
    "        #TODO\n",
    "    return net, ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = lr_inc_dec(lr_0=0.01, lr_0_steps=10, lr_max = 0.03, lr_max_steps = 5, lr_min=0.02, lr_decay=0.85)\n",
    "DATA = {'Input': P, 'Target': T}\n",
    "PERF = mse()\n",
    "MAX_EPOCH = 30\n",
    "NET = simplenet(0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET2, FF = train_gd(NET, PERF, lr_scheduler, MAX_EPOCH, DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the progress of the performance function during training. The performance should decay toward 0. You could try changing the learning rate scheduler to see how it affects the convergence of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(MAX_EPOCH), FF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas and a Data Generator\n",
    "\n",
    "Now consider a little more complex problem, in which we load some data from a file into a Pandas DataFrame, transform the data, and then set up a data generator to read the data into mini-batches to train the network.\n",
    "\n",
    "To begin, we read a CSV file into a DataFrame. This is the same file used in the Python Chapter Jupyter Notebook. For this task we will be trying to predict the **Percent** value from the **FVC** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv('https://raw.githubusercontent.com/NNDesignDeepLearning/NNDesignDeepLearning/master/05.PythonChapter/Code/ChapterNotebook/SampleDF.csv?token=AUZTTDQALCLLAYUY2ZZGDKDA5YTEK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training neural networks we commonly normalize the input and target data before training the network, as described in Chapter 4 of the textbook. For this problem let's normalize the inputs and targets to have a mean of 0 and a standard deviation of 1. In order to do that, compute the mean and standard deviation of the **FVC** and **Percent** columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fvc = #TODO\n",
    "std_fvc = #TODO\n",
    "mean_percent = #TODO\n",
    "std_percent = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform the **FVC** and **Percent** columns so that they have a mean of 0 and a standard deviation of 1. Use the `apply` method and a `lambda` function that subtracts the mean and divides by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['FVC'] = #TODO\n",
    "sample_df['Percent'] = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the input variable `P` to the **FVC** column of the DataFrame and the target variable `T` to the **Percent** column of the DataFrame, after converting them to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = #TODO\n",
    "T = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As earlier, put the inputs and targets into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = {'Input': P, 'Target': T}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "We covered generators in the Python Chapter Jupyter Notebook. Let's make a generator that will be passed into the training function, which will use it to return mini-batches of the training data. The generator should continue to pass through the data set an indefinite number of times. It will shuffle the data set after each epoch. If the length of the data set is not an integer multiple of the batch size, some remainder of data points will be left out of each epoch, but because of shuffling, all data will eventually be included in training.\n",
    "\n",
    "There will be two inputs to the generator: 1) the dictionary containing the data set and 2) the mini-batch size `bsize`. The generator should then extract the input and target from the dictionary and iterate through the data, extracting `bsize` columns from the input and target arrays at each iteration. \n",
    "\n",
    "On each iteration, the data generator should yield three things: 1) the epoch number, 2) `bsize` columns of the input array and 3) `bsize` columns of the target array. At the completion of each epoch, it should randomly reorder the columns of the input and target arrays, so that they will be presented in a different order on each epoch. (Hint: The function `np.random.permutation` can be useful in reordering the arrays. Be sure to use the same indexing for both inputs and targets.) The generator should be written so that it can be used for an arbitrary number of epochs. The training function will know when to stop accessing the generator when the epoch number reaches the desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(data, bsize):\n",
    "    In = data['Input']\n",
    "    Tar = data['Target']\n",
    "    num = len(In)\n",
    "    steps = num//bsize\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        for ii in range(steps):\n",
    "            yield epoch, #TODO\n",
    "        #TODO\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Training Function\n",
    "\n",
    "Modify the earlier training function, `train_gd`, so that it's fifth input is a data generator, rather than the data set itself. It should still run for `max_epoch` epochs, but there will be multiple iterations for each epoch if the mini-batch size is smaller than the full data set size. \n",
    "\n",
    "The form of the `train_gd_gen` function will be almost identical to the `train_gd` function. However, instead of incrementing the epoch number, it will call the data generator with the `next` command, which will return the current epoch number. This epoch number will be used to access the correct learning rate.\n",
    "\n",
    "Except for the data generator, the inputs and outputs of `train_gd_gen` will be the same as those of `train_gd`. However, the returned list of performance values will be provided at each iteration, which could be multiple times per epoch, depending on the mini-batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gd_gen(net, perf, lr_sched, max_epoch, datagen):\n",
    "    ff = []\n",
    "    ep = 0\n",
    "    while ep  < max_epoch:\n",
    "        #TODO\n",
    "    return net, ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the data generator and the new training function. In the values set below the mini-batch size is set to 10, which means that there will be 10 iterations per epoch, since there are 100 data points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = lr_inc_dec(lr_0=0.001, lr_0_steps=10, lr_max = 0.01, lr_max_steps = 10, lr_min=0.000001, lr_decay=0.85)\n",
    "DATA = {'Input': P, 'Target': T}\n",
    "BSIZE = 10\n",
    "PERF = mse()\n",
    "MAX_EPOCH = 50\n",
    "NET = simplenet(0.1, 0.1)\n",
    "gendata = data_gen(DATA, BSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET2, FF = train_gd_gen(NET, PERF, lr_scheduler, MAX_EPOCH, gendata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the performance during training. You will notice that the plot is not as smooth as the earlier training example, which used full batch training. Since at each iteration the performance is computed over only a subset of the data, the convergence can be noisy. You can experiment with larger mini-batch sizes to see how this effects the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(FF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how well the final trained network fit the training data. Here we find the network response to inputs over the range -3 to 3 (which is the approximate range of the normalized **FVC** values) and compare it to the training data. Since we only use a one layer network, the response of the network is linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.arange(-3,3,0.1)\n",
    "aa = NET2.sim(pp)\n",
    "plt.plot(pp, aa, 'r', P, T, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Further\n",
    "\n",
    "Experiment with different learning rate schedules and mini-batch sizes to see how they affect training. \n",
    "\n",
    "Implement other training algorithms, such as Adam. \n",
    "\n",
    "Implement a two-layer network. \n",
    "\n",
    "Load your own data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
